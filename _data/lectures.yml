- date: 1
  title: >
    <strong>Introduction</strong> <a href="https://drive.google.com/file/d/1xtXj95tJeBGxD4quXCDYoKqyQTg-BoUf/view?usp=sharing">[slides]</a> <a href="https://screencast-o-matic.com/watch/c31trKV06YZ">[video]</a>
  slides:
  topics:
    - What is Multimodal? Definitions, dimensions of heterogeneity and cross-modal interactions.<br/>
    - Historical view and multimodal research tasks.<br/>
    - Core technical challenges&#58; representation, alignment, transference, reasoning, generation, and quantification.<br/>
  readings:
    - <a href="https://arxiv.org/abs/1705.09406">Multimodal Machine Learning&#58; A Survey and Taxonomy</a> <br/>
    - <a href="https://arxiv.org/abs/1206.5538">Representation Learning&#58; A Review and New Perspectives</a> <br/>

- date: 2
  title: >
    <strong>Representation</strong> <a href="https://drive.google.com/file/d/1JzByYECmzlloWKjcPsmRrLCj-QngGWLD/view?usp=sharing">[slides]</a> <a href="https://screencast-o-matic.com/watch/c31t3rV06Ze">[video]</a>
  topics:
    - Representation fusion&#58; additive, multiplicative, non-linear, complex fusion strategies.<br/>
    - Representation coordination&#58; contrastive learning, vector-space models, canonical correlation analysis.<br/>
    - Representation fission&#58; factorization, component analysis, clustering.<br/>
  readings:
    - <a href="https://openreview.net/pdf?id=rylnK6VtDH">Multiplicative Interactions and Where to Find Them</a> <br/>
    - <a href="https://jmlr.org/papers/volume15/srivastava14b/srivastava14b.pdf">Multimodal Learning with Deep Boltzmann Machines</a> <br/>    
    - <a href="https://arxiv.org/abs/1707.07250">Tensor Fusion Network for Multimodal Sentiment Analysis</a> <br/>
    - <a href="https://arxiv.org/abs/1702.01992">Gated Multimodal Units for Information Fusion</a> <br/>
    - <a href="https://aclanthology.org/2020.emnlp-main.62/">Does My Multimodal Model Learn Cross-modal Interactions? It’s Harder to Tell Than You Might Think!</a> <br/>
    - <a href="https://arxiv.org/abs/1602.01024">On Deep Multi-View Representation Learning&#58; Objectives and Optimization</a> <br/>
    - <a href="https://arxiv.org/abs/1411.2539">Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a> <br/>
    - <a href="https://arxiv.org/abs/1806.06176">Learning Factorized Multimodal Representations</a> <br/>
    - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8336846">Multi-view Clustering&#58; A Survey</a> <br/>
    
- date: 3
  title: >
    <strong>Alignment</strong> <a href="https://drive.google.com/file/d/1Vku7SqKQiu2BTQ5O-DD9sYUmelno2yrd/view?usp=sharing">[slides]</a> <a href="https://screencast-o-matic.com/watch/c31u3gV0Iyh">[video]</a>
  topics:
    - Connections&#58; grounding, optimal transport, distribution matching.<br/>
    - Aligned representations&#58; attention models, multimodal transformers, graph neural networks.<br/>
    - Segmentation&#58; time warping, CTC, temporal alignment, clustering<br/>
  readings:
    - <a href="https://home.ttic.edu/~klivescu/papers/andrew_icml2013.pdf">Deep Canonical Correlation Analysis</a> <br/>
    - <a href="https://arxiv.org/abs/2006.14744">Graph Optimal Transport for Cross-domain Alignment</a> <br/>
    - <a href="https://ibug.doc.ic.ac.uk/media/uploads/documents/deep_canonical_time_warping_(1).pdf">Deep Canonical Time Warping for Simultaneous Alignment and Representation Learning of Sequences</a> <br/>
    - <a href="https://arxiv.org/abs/1412.2306">Deep Visual-semantic Alignments for Generating Image Descriptions</a> <br/>
    - <a href="https://arxiv.org/abs/2012.02813">Cross-Modal Generalization&#58; Learning in Low Resource Modalities via Meta-Alignment</a> <br/>
    - <a href="https://arxiv.org/abs/1908.02265">ViLBERT&#58; Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a> <br/>
    - <a href="https://arxiv.org/abs/1906.00295">Multimodal Transformer for Unaligned Multimodal Language Sequences</a> <br/>
    - <a href="https://arxiv.org/abs/2102.00529">Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers</a> <br/>
  
- date: 4
  title: >
    <strong>Reasoning</strong> <a href="https://drive.google.com/file/d/1A_brIgLDT1GRWJRiZXn9VrRgZLazKqKn/view?usp=sharing">[slides]</a> <a href="https://screencast-o-matic.com/watch/c31u2wV0l3l">[video]</a>
  topics:
    - Structure&#58; hierarchical, graphical, temporal, and interactive structure, structure discovery.<br/>
    - Concepts&#58; dense and neuro-symbolic.<br/>
    - Inference&#58; logical and causal inference.<br/>
    - Knowledge&#58; external knowledge bases, commonsense reasoning.<br/>
  readings:
    - <a href="https://arxiv.org/abs/1906.01784">Learning to Compose and Reason with Language Tree Structures for Visual Grounding</a> <br/>
    - <a href="https://arxiv.org/abs/1904.12584">The Neuro-Symbolic Concept Learner&#58; Interpreting Scenes, Words, and Sentences From Natural Supervision</a> <br/>
    - <a href="https://arxiv.org/abs/1906.03926">A Survey of Reinforcement Learning Informed by Natural Language</a> <br/>
    - <a href="https://arxiv.org/abs/1603.01417">Dynamic Memory Networks for Visual and Textual Question Answering</a> <br/>
    - <a href="https://arxiv.org/abs/1611.05592">Multimodal Memory Modelling for Video Captioning</a> <br/>
    - <a href="https://aclanthology.org/D18-1280.pdf">ICON&#58; Interactive Conversational Memory Network for Multimodal Emotion Detection</a> <br/>
    - <a href="https://arxiv.org/abs/2002.08325">VQA-LOL&#58; Visual Question Answering Under the Lens of Logic</a> <br/>
    - <a href="https://arxiv.org/abs/1912.07538">Towards Causal VQA&#58; Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing</a> <br/>
    - <a href="https://arxiv.org/abs/1507.05670">Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries</a> <br/>
    - <a href="https://arxiv.org/abs/2112.08614">KAT&#58; A Knowledge Augmented Transformer for Vision-and-Language</a> <br/>

- date: 5
  title: >
    <strong>Generation</strong> <a href="https://drive.google.com/file/d/13c4sS80N6rzb1rp_NynhSo4B0GiA2tkx/view?usp=sharing">[slides]</a> <a href="https://screencast-o-matic.com/watch/c31u2GV0lZa">[video]</a>
  topics:
    - Summarization, translation, and creation.<br/>
    - Model evaluation and ethical concerns.<br/>
  readings:
    - <a href="https://arxiv.org/abs/2102.10407">VisualGPT&#58; Data-efficient Adaptation of Pretrained Language Models for Image Captioning</a> <br/>
    - <a href="https://openai.com/blog/dall-e/">DALL·E&#58; Creating Images from Text</a> and <a href="https://openai.com/dall-e-2/">DALL·E 2</a> <br/>
    - <a href="https://dl.acm.org/doi/abs/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots&#58; Can Language Models Be Too Big?</a> <br/>
    - <a href="https://www.liebertpub.com/doi/full/10.1089/cyber.2021.29208.jth">The Social Impact of Deepfakes</a> <br/>
    - <a href="https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias">What a machine learning tool that turns Obama white can (and can’t) tell us about AI bias</a> <br/>
    - <a href="https://arxiv.org/abs/2005.03201">What comprises a good talking-head video generation?&#58; A Survey and Benchmark</a> <br/>
    - <a href="https://arxiv.org/abs/1905.12616">Defending Against Neural Fake News</a> <br/>
    - <a href="https://thegradient.pub/pulse-lessons/">Lessons from the PULSE Model and Discussion</a> <br/>
    - <a href="https://arxiv.org/abs/2005.00908">Cross-modal Coherence Modeling for Caption Generation</a> <br/>
    - <a href="https://arxiv.org/abs/1906.07901">Multimodal Abstractive Summarization for How2 Videos</a> <br/>
    - <a href="https://arxiv.org/abs/2012.07805">Extracting Training Data from Large Language Models</a> <br/>
    - <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Suwajanakorn_What_Makes_Tom_ICCV_2015_paper.pdf">What Makes Tom Hanks Look Like Tom Hanks</a> <br/>
    - <a href="https://arxiv.org/abs/1710.00421">Video Generation From Text</a> <br/>

- date: 6
  title: >
    <strong>Transference</strong> <a href="https://drive.google.com/file/d/1Wazw91eU3wjguHNTqzUX1zmFZYqjlIR4/view?usp=sharing">[slides]</a> <a href="https://screencast-o-matic.com/watch/c31uoOV0lx1">[video]</a>
  topics:
    - Transfer via pre-trained models&#58; pre-trained models, prefix tuning, representation tuning, multitask models.<br/>
    - Co-learning&#58; co-learning via representation and generation.<br/>
  readings:
    - <a href="https://arxiv.org/abs/2010.06775">Vokenization&#58; Improving Language Understanding via Contextualized, Visually-Grounded Supervision</a> <br/>
    - <a href="https://www.sciencedirect.com/science/article/pii/S1566253520303006">Foundations of Multimodal Co-learning</a> <br/>
    - <a href="https://arxiv.org/abs/2011.08899">Multimodal Prototypical Networks for Few-shot Learning</a> <br/>
    - <a href="https://arxiv.org/abs/2103.05677">SMIL&#58; Multimodal Learning with Severely Missing Modality</a> <br/>
    - <a href="https://arxiv.org/abs/2107.13782">Multimodal Co-learning&#58; Challenges, Applications with Datasets, Recent Advances and Future Directions</a> <br/>
    - <a href="https://arxiv.org/abs/2106.04538">What Makes Multi-modal Learning Better than Single (Provably)</a> <br/>
    - <a href="https://arxiv.org/abs/1812.07809">Found in Translation&#58; Learning Robust Joint Representations by Cyclic Translations Between Modalities</a> <br/>
    - <a href="https://arxiv.org/abs/1301.3666">Zero-Shot Learning Through Cross-Modal Transfer</a> <br/>
    - <a href="https://arxiv.org/abs/1912.02315">12-in-1&#58; Multi-Task Vision and Language Representation Learning</a> <br/>
    - <a href="https://arxiv.org/abs/1906.03926">A Survey of Reinforcement Learning Informed by Natural Language</a> <br/>

- date: 7
  title: >
    <strong>Quantification</strong> <a href="https://drive.google.com/file/d/1Df0uS61_aFAc1zaXP-cijsqzOng4kX7A/view?usp=sharing">[slides]</a> <a href="https://screencast-o-matic.com/watch/c31uDYV0lkN">[video]</a>
  topics:
    - Dimensions of heterogenity&#58; modality importance, dataset biases, social biases, noise topologies and robustness.<br/>
    - Cross-modal interactions&#58; interpreting cross-model connections and interactions.<br/>
    - Learning&#58; learning and optimization challenges.<br/>
  readings:
    - <a href="https://arxiv.org/abs/2107.07502">MultiBench&#58; Multiscale Benchmarks for Multimodal Representation Learning</a> <br/>
    - <a href="https://arxiv.org/abs/2203.01311">HighMMT&#58; Towards Modality and Task Generalization for High-Modality Representation Learning</a> <br/>
    - <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Tran_Missing_Modalities_Imputation_CVPR_2017_paper.html">Missing Modalities Imputation via Cascaded Residual Autoencoder</a> <br/>
    - <a href="https://arxiv.org/abs/2107.08264">M2Lens&#58; Visualizing and Explaining Multimodal Models for Sentiment Analysis</a> <br/>
    - <a href="https://arxiv.org/abs/2203.17247">VL-InterpreT&#58; An Interactive Visualization Tool for Interpreting Vision-Language Transformers</a> <br/>
    - <a href="https://arxiv.org/abs/1606.03490">The Mythos of Model Interpretability</a> <br/>
    - <a href="https://arxiv.org/abs/2103.06254">Interpretable Machine Learning&#58; Moving From Mythos to Diagnostics</a> <br/>
    - <a href="https://arxiv.org/abs/2202.01602">The Disagreement Problem in Explainable Machine Learning&#58; A Practitioner's Perspective</a> <br/>
    - <a href="https://arxiv.org/abs/1810.12366">Do explanations make VQA models more predictable to a human?</a> <br/>
    - <a href="https://arxiv.org/abs/1803.09797">Women also Snowboard&#58; Overcoming Bias in Captioning Models</a> <br/>
    - <a href="https://aclanthology.org/2021.naacl-main.78.pdf">Measuring Social Biases in Grounded Vision and Language Embeddings</a> <br/>
    - <a href="https://arxiv.org/abs/2110.01963">Multimodal datasets&#58; misogyny, pornography, and malignant stereotypes</a> <br/>
    
